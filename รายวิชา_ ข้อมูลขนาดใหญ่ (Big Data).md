# รายวิชา: ข้อมูลขนาดใหญ่ (Big Data)
ชื่อ สหรัฐ สมดา
รหัสวิชา / หน่วยกิต: 3(2–2–5)  
ระดับการศึกษา: ปริญญาตรี ชั้นปีที่ 2  
คณะ: คณะวิทยาการคอมพิวเตอร์ มหาวิทยาลัยสยาม  
เวลาบรรยาย–ปฏิบัติ: บรรยาย 2 ชั่วโมง/สัปดาห์ และปฏิบัติการ 2 ชั่วโมง/สัปดาห์  
วิชาบังคับก่อน: ไม่มี

# คำอธิบายรายวิชา (Course Description)

ศึกษาหลักการ แนวคิด และองค์ประกอบสำคัญของการประมวลผลข้อมูลขนาดใหญ่ รวมถึงระบบนิเวศ (ecosystem) ของ Big Data ทั้งในด้านการจัดเก็บข้อมูล การประมวลผลแบบกระจาย การวิเคราะห์ข้อมูลเชิงปริมาณ และการนำเสนอผลลัพธ์ โดยให้นักศึกษาสามารถทำความเข้าใจโครงสร้างพื้นฐานของระบบ Big Data ในระดับภาพรวม สร้างกระบวนการจัดการข้อมูล (data pipeline) พื้นฐานได้ด้วยตนเอง และประยุกต์ใช้เครื่องมือและเทคโนโลยีที่ทันสมัยเพื่อวิเคราะห์ชุดข้อมูลขนาดใหญ่ พร้อมทั้งสื่อสารผลลัพธ์ในลักษณะของแผงควบคุม (dashboard) และบทสรุปเชิงวิเคราะห์อย่างเหมาะสม

## วัตถุประสงค์การเรียนรู้ (Intended Learning Outcomes – ILOs)

เมื่อสิ้นสุดรายวิชา นักศึกษาจะสามารถ:  
อธิบายหลักการและองค์ประกอบของระบบ Big Data ได้แก่ ระบบจัดเก็บข้อมูลรูปแบบใหม่ (Parquet, Columnar Storage), ระบบประมวลผลแบบกระจาย (เช่น Apache Spark), ระบบฐานข้อมูลแบบ SQL/NoSQL และการประมวลผลแบบสตรีมมิง ได้อย่างถูกต้องและเป็นระบบ  
สร้างและดำเนินงานกระบวนการจัดการข้อมูล (Data Pipeline) พื้นฐานสำหรับชุดข้อมูลที่มีขนาดใหญ่กว่าเครื่องมือทั่วไป เช่น Excel โดยสามารถดำเนินขั้นตอน ingest, clean, transform, store และ analyze ได้อย่างถูกต้อง  
ใช้เครื่องมือ PySpark, Pandas, PyArrow และระบบไฟล์แบบ Parquet เพื่อประมวลผลชุดข้อมูลปริมาณมาก พร้อมสามารถวัดเวลาในการประมวลผลและเปรียบเทียบประสิทธิภาพระหว่างรูปแบบไฟล์ต่าง ๆ ได้  
วิเคราะห์ข้อมูลเชิงปริมาณเบื้องต้น สร้างตัวชี้วัด (KPI) ที่เหมาะสม และนำเสนอผลลัพธ์ในรูปแบบ dashboard ด้านข้อมูล พร้อมอธิบายความหมายทางบริบทและข้อจำกัดของข้อมูล  
ใช้ระบบปัญญาประดิษฐ์เชิงกำเนิด (Generative AI) เป็นเครื่องมือสนับสนุนการทำงาน โดยสามารถออกแบบคำสั่ง (prompt) ที่เหมาะสม ตรวจสอบความน่าเชื่อถือของคำตอบ และปรับใช้ผลลัพธ์อย่างมีวิจารณญาณ  
ปฏิบัติตามหลักจริยธรรมในการใช้ข้อมูล โดยเฉพาะข้อมูลที่มีความอ่อนไหว (PII) และสามารถระบุความเสี่ยงด้านคุณภาพข้อมูล อคติของข้อมูล และข้อจำกัดของชุดข้อมูลที่นำมาใช้ในการวิเคราะห์

## 

## แผนการสอนรายสัปดาห์ (Course Schedule)

ตารางนี้จัดทำในรูปแบบภาษาทางการ พร้อมวัตถุประสงค์/เนื้อหา/กิจกรรมปฏิบัติในแต่ละสัปดาห์

### สัปดาห์ที่ 1: บทนำสู่ Big Data และคุณลักษณะ 5Vs

เนื้อหา: ความหมายของ Big Data, ปัญหาของข้อมูลขนาดใหญ่, กรณีศึกษาการประยุกต์ใช้  
ปฏิบัติการ: วิเคราะห์ชุดข้อมูลเบื้องต้นด้วย Pandas

### สัปดาห์ที่ 2: รูปแบบไฟล์ข้อมูลและสถาปัตยกรรมการจัดเก็บสมัยใหม่

เนื้อหา: CSV, Parquet, Columnar Storage, แนวคิด Data Lake และ Lakehouse  
ปฏิบัติการ: แปลง CSV → Parquet และทดสอบการอ่านข้อมูล (time/size comparison)

### สัปดาห์ที่ 3: การใช้ SQL เพื่อวิเคราะห์ข้อมูลขนาดใหญ่

เนื้อหา: การประมวลผลข้อมูลด้วย SQL, DuckDB/Spark SQL  
ปฏิบัติการ: การใช้คำสั่งพื้นฐานสำหรับการสรุปข้อมูล

### สัปดาห์ที่ 4: แนวคิดระบบประมวลผลแบบกระจายและ Apache Spark

เนื้อหา: Spark Architecture, DataFrame API, การดำเนินงานแบบ Lazy Evaluation  
ปฏิบัติการ: การประมวลผลข้อมูลระดับหลายล้านแถวด้วย Spark

### สัปดาห์ที่ 5: กระบวนการจัดการข้อมูล (Data Pipeline) เบื้องต้น

เนื้อหา: ingest – clean – transform – load – analyze  
ปฏิบัติการ: สร้าง pipeline ขนาดย่อมใน Colab

### สัปดาห์ที่ 6: ฐานข้อมูลแบบ NoSQL และการจัดเก็บข้อมูลที่ไม่มีโครงสร้าง

เนื้อหา: Document Store, Columnar DB, Key-value Store และกรณีการใช้งาน  
ปฏิบัติการ: วิเคราะห์ข้อมูล JSON ตัวอย่าง

### สัปดาห์ที่ 7: การประมวลผลข้อมูลสตรีมมิง (Streaming Analytics) – ภาพรวม

เนื้อหา: แนวคิด Event Stream, Window Aggregation  
ปฏิบัติการ: จำลองการประมวลผลข้อมูลแบบไหลเข้าอย่างต่อเนื่อง

### สัปดาห์ที่ 8: การวิเคราะห์ข้อมูลเบื้องต้น (Exploratory Data Analysis – EDA)

ปฏิบัติการ: การแสดงผลข้อมูล, การจัดทำ KPI พื้นฐาน และการสร้างกราฟเพื่ออธิบายแนวโน้ม

### สัปดาห์ที่ 9: การเรียนรู้ของเครื่องเบื้องต้นในบริบทข้อมูลขนาดใหญ่

เนื้อหา: แนวคิด train-test, การจำแนก/การพยากรณ์  
ปฏิบัติการ: ทดลองใช้งานโมเดลสำเร็จรูป

### สัปดาห์ที่ 10: การประกันคุณภาพข้อมูล (Data Quality Assurance)

เนื้อหา: ความคลาดเคลื่อนของข้อมูล, ความสมบูรณ์ของข้อมูล  
ปฏิบัติการ: การสร้างกฎคุณภาพข้อมูลขั้นพื้นฐาน

### สัปดาห์ที่ 11: จริยธรรมและความเป็นส่วนตัวของข้อมูล (Data Ethics & Privacy)

เนื้อหา: PII, การมาส์กข้อมูล, ข้อจำกัดตามมาตรฐานข้อมูลสาธารณะ  
ปฏิบัติการ: วิเคราะห์ความเสี่ยงของชุดข้อมูลที่เลือกใช้

### สัปดาห์ที่ 12: ระบบควบคุมเวิร์กโฟลว์ข้อมูล (Orchestration Systems)

เนื้อหา: ภาพรวม Apache Airflow และ DAG  
ปฏิบัติการ: อ่าน/อธิบาย DAG ตัวอย่าง

### สัปดาห์ที่ 13: ประสิทธิภาพและต้นทุนของระบบข้อมูล (Performance & Cost Considerations)

ปฏิบัติการ: เปรียบเทียบประสิทธิภาพ CSV vs Parquet และผลของการทำ Partition/Compression

### สัปดาห์ที่ 14: สัปดาห์เตรียมโครงงาน (Project Workshop)

ตรวจโครงสร้างรีโพสาธารณะ (GitHub), ตรวจความสามารถในการรันซ้ำ (reproducibility), ตรวจ dashboard

### สัปดาห์ที่ 15: นำเสนอผลงานและสอบปลายภาค

นำเสนอ Final Project ต่อคณะผู้สอน  
สอบปลายภาค (เปิดตำรา/เปิด AI พร้อมระบุ prompt และเหตุผลประกอบ)

## รูปแบบการประเมินผลการเรียน (Assessment Criteria)

#### การประเมินผลจากโครงงานปลายภาค (Final Project) — 60%

นักศึกษาจะต้องเลือกหนึ่งในสามระดับความยาก (ขั้นสูง/กลาง/พื้นฐาน) โดยใช้ชุดข้อมูลด้านอาชญากรรม (Crime Dataset) ขนาด ≥ 200MB จากเมืองใดเมืองหนึ่ง (Chicago/SF/NYC/LA) และดำเนินการตามกระบวนการ pipeline จนถึงการจัดทำ dashboard  
องค์ประกอบการประเมิน ได้แก่:  
ความครบถ้วนของ data pipeline (ingest → clean → Parquet/Arrow → analyze) 15%  
Dashboard และความถูกต้องของการวิเคราะห์ 15%  
การทดสอบประสิทธิภาพและการเปรียบเทียบขนาด/เวลา (benchmark) 10%  
ความถูกต้องของเอกสาร, README, โครงสร้างรีโพสาธารณะ 10%  
การนำเสนอผลงานและการตอบคำถามปากเปล่า 10%

#### การสอบกลางภาค — 15%

ข้อสอบเพื่อประเมินความเข้าใจในหลักการพื้นฐานของ Big Data, SQL, Parquet และสถาปัตยกรรมระบบ

#### การสอบปลายภาค — 15%

รูปแบบ Open Notes & Open AI ผู้เรียนต้องแนบ prompt และเหตุผลประกอบทุกคำตอบ

#### การเข้าเรียนและการมีส่วนร่วม — 10%

ผ่านการเข้าร่วมปฏิบัติการ, กิจกรรมย่อย (mini-labs), และการทำแบบฝึกหัดสั้นท้ายคาบ

## สื่อการเรียนรู้ (Learning Resources)

### ตำราหลัก

Kleppmann, M. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems.  
Reis, J., & Housley, M. Fundamentals of Data Engineering: Plan and Build Robust Data Systems.  
ตำราและเอกสารเพิ่มเติม  
Chambers, B., & Zaharia, M. Spark: The Definitive Guide.  
เอกสารคู่มือ DuckDB, PyArrow, Apache Spark, Streamlit, Plotly (ออนไลน์)

## นโยบายด้านจริยธรรมทางวิชาการ

อนุญาตให้ใช้เครื่องมือปัญญาประดิษฐ์เพื่อสนับสนุนการเรียนรู้และการทำโครงงานได้ แต่ต้องระบุ prompt ทั้งหมดที่ใช้ และอธิบายส่วนที่พัฒนาด้วยตนเองอย่างชัดเจน  
ห้ามคัดลอกผลงานของผู้อื่น หรือให้ผู้อื่นทำงานแทน  
ผู้สอนสงวนสิทธิ์ในการเรียกนักศึกษาชี้แจงผลงานแบบปากเปล่า หากไม่อาจอธิบายได้ถือว่าผลงานไม่ผ่าน  
เงื่อนไขรายงานการส่งงานบน GitHub (Public Repository Requirements)  
นักศึกษาต้องจัดทำรีโพสาธารณะซึ่งประกอบด้วย:  
README อธิบายแหล่งข้อมูล วิธีรันกระบวนการ วิธีแสดง dashboard  
Notebook/Scripts (ingest/clean/store/analyze)  
โครงสร้างแฟ้มข้อมูลที่เหมาะสม (ไม่อัปโหลดไฟล์ขนาดใหญ่ ให้แนบสคริปต์ดาวน์โหลดแทน)  
ตารางผลการทดสอบ benchmark  
เอกสารประกอบเกี่ยวกับข้อจำกัดข้อมูลและประเด็นจริยธรรม  
วิดีโอสาธิต (ไม่เกิน 5 นาที)
